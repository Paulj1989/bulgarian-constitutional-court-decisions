{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "\n",
    "- Using three baseline models for the development of an analysis that detects political language in court documents.\n",
    "    1. Logistic regression\n",
    "    2. Naive Bayes\n",
    "    3. SVM\n",
    "- Using baselines as a means for testing model performance and building an accurate, model for classifying language used in Bulgaria's constitutional court\n",
    "\n",
    "## Next steps\n",
    "\n",
    "- Data needs more sentences labelled as political, as data is imbalanced and models observe few political sentences\n",
    "- Need to optimize hyperparameters to improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import naive_bayes, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all json files in data directory\n",
    "files = glob.glob(\"data/json/*.json\")\n",
    "data = []\n",
    "\n",
    "# for loop for processing files and adding doc id\n",
    "for json in files:\n",
    "    frame = pd.read_json(json)\n",
    "    # get file name as string\n",
    "    # create column identifying dfs as doc_id\n",
    "    # split string (remove .json from file name)\n",
    "    frame[\"doc_id\"] = os.path.splitext(os.path.basename(json))[0]\n",
    "    data.append(frame)\n",
    "\n",
    "# concatenate all data frames\n",
    "df = pd.concat(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary variable where POLITICAL = 1, all else = 0\n",
    "df.loc[df[\"label_id\"] != 4, \"label_id\"] = 0\n",
    "\n",
    "df.loc[df[\"label_id\"] == 4, \"label_id\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in text.lower().split() if not word in stop_words]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def token_ps(text):\n",
    "    return [ps.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Computing a logistic regression model based on the values created from a vectorizer algorithm called tf-idf, which stands for term-frequency inverse document frequency.\n",
    "- tf-idf measures the originality of the word by comparing how often it appears in a doc with the number of docs the word appears in. The frequency of the words in a doc (compared against other docs) measures the importance of that word in the wider corpus.\n",
    "- The logistic regression below is computed by building a vector of word values based on the iportance of each word, before using the word vectors to identify the characteristics of the political label to predict which sentences will be political."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming text into vectors\n",
    "tfidf = TfidfVectorizer(lowercase=False,\n",
    "                        use_idf=True,\n",
    "                        norm='l2',\n",
    "                        smooth_idf=True)\n",
    "# compute tfidf values for all words in 'text' column of df\n",
    "X = tfidf.fit_transform(df['text'])\n",
    "y = df.label_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    4.1s remaining:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   8 | elapsed:    4.8s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    5.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    5.1s finished\n"
     ]
    }
   ],
   "source": [
    "# splitting data into train and test splits in order to test predictive accuracy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0, shuffle=True\n",
    ")\n",
    "\n",
    "# computes and then fits logistic regression that implements cross-validation as a part of the process\n",
    "# cv = number of cross validation folds\n",
    "log_reg = LogisticRegressionCV(\n",
    "    cv=8, scoring=\"accuracy\", n_jobs=-1, verbose=3\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# model accuracy\n",
    "log_predictions = log_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function that prints model prediction accuracy\n",
    "def model_accuracy(name, preds):\n",
    "    print(\"---{} Test Set Results---\".format(name))\n",
    "    print(\"Weighted F1 Average: {}\".format(f1_score(y_test, preds, average=\"weighted\")))\n",
    "    # precision = % predicted accurately\n",
    "    # recall = % positives identified\n",
    "    # f1-score = weighted harmonic mean of precision & recall\n",
    "    # weighted f-1 avg used for comparing classification models\n",
    "    print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Logit Test Set Results---\n",
      "Weighted F1 Average: 0.8000997426850943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       127\n",
      "           1       1.00      0.05      0.09        22\n",
      "\n",
      "    accuracy                           0.86       149\n",
      "   macro avg       0.93      0.52      0.51       149\n",
      "weighted avg       0.88      0.86      0.80       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_accuracy(\"Logit\", log_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Naive Bayes Test Set Results---\n",
      "Weighted F1 Average: 0.8372325419305285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       127\n",
      "           1       0.47      0.36      0.41        22\n",
      "\n",
      "    accuracy                           0.85       149\n",
      "   macro avg       0.68      0.65      0.66       149\n",
      "weighted avg       0.83      0.85      0.84       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "nb = naive_bayes.ComplementNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# model accuracy\n",
    "nb_predictions = nb.predict(X_test)\n",
    "model_accuracy(\"Naive Bayes\", nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SVM Test Set Results---\n",
      "Weighted F1 Average: 0.8147422394225902\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93       127\n",
      "           1       1.00      0.09      0.17        22\n",
      "\n",
      "    accuracy                           0.87       149\n",
      "   macro avg       0.93      0.55      0.55       149\n",
      "weighted avg       0.88      0.87      0.81       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "SVM = svm.SVC(C=1, kernel='linear', class_weight='balanced')\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "# model accuracy\n",
    "svm_predictions = SVM.predict(X_test)\n",
    "model_accuracy(\"SVM\", svm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling Models (for Future Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tfidf\n",
    "pickle.dump(tfidf, open('tfidf.pickle', 'wb'))\n",
    "\n",
    "# saving models\n",
    "pickle.dump(log_reg, open('log_reg.pickle', 'wb'))\n",
    "pickle.dump(nb, open('nb.pickle', 'wb'))\n",
    "pickle.dump(SVM, open('svm.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('bulgarian-constitutional-court-decisions': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "9e35bad0b5aa7a87571eaa6bfdcd491c11984b99cb58639a3f82d97f7d8977e2"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}