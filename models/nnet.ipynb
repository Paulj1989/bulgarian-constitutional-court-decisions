{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM, Conv1D, Dense, Embedding, Flatten, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing Text Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all json files in data directory\n",
    "files = glob.glob(\"data/json/*.json\")\n",
    "data = []\n",
    "\n",
    "# for loop for processing files and adding doc id\n",
    "for json in files:\n",
    "    frame = pd.read_json(json)\n",
    "    # get file name as string\n",
    "    # create column identifying dfs as doc_id\n",
    "    # split string (remove .json from file name)\n",
    "    frame[\"doc_id\"] = os.path.splitext(os.path.basename(json))[0]\n",
    "    data.append(frame)\n",
    "\n",
    "# concatenate all data frames\n",
    "df = pd.concat(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary variable where POLITICAL = 1, all else = 0\n",
    "df.loc[df[\"label_id\"] != 4, \"label_id\"] = 0\n",
    "\n",
    "df.loc[df[\"label_id\"] == 4, \"label_id\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocessing text data\n",
    "def preprocessing(text):\n",
    "\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in text.lower().split() if not word in stop_words]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying preprocessing function to df\n",
    "df['text'] = df['text'].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Word Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector parameters\n",
    "vocab_size = 3500\n",
    "max_length = 1000\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text\n",
    "y = df.label_id.values\n",
    "# splitting data into train and test splits in order to test predictive accuracy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.3, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# fitting tokenizer only to training set\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# creating training sequences and padding them\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "train_pad = pad_sequences(\n",
    "    train_seq,\n",
    "    maxlen=max_length,\n",
    "    padding=padding_type,\n",
    "    truncating=trunc_type,\n",
    ")\n",
    "\n",
    "# creating testing sequences and padding them using same tokenizer\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "test_pad = pad_sequences(\n",
    "    test_seq,\n",
    "    maxlen=max_length,\n",
    "    padding=padding_type,\n",
    "    truncating=trunc_type,\n",
    ")\n",
    "\n",
    "# converting all variables to numpy arrays (correct format for the latest version of tensorflow)\n",
    "train_seq = np.array(train_seq)\n",
    "train_pad = np.array(train_pad)\n",
    "y_train = np.array(y_train)\n",
    "test_seq = np.array(test_seq)\n",
    "test_pad = np.array(test_pad)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Convolutional Neural Network\n",
    "\n",
    "Convolutional neural networks (CNNs) are designed for processing large arrays of structured data. They are most popular for computer vision tasks, but they can be effectively applied to natural language processing in certain cases, particularly text classification.\n",
    "\n",
    "The architecture of a CNN is a multi-layered \"feed-forward\" neural network. A feed-forward network is a network whose nodes do not form a cycle (an example of a network that forms a cycle is a recurrent neural network).\n",
    "\n",
    "A diagram example of a convolutional neural network helps demonstrate how the process works:\n",
    "\n",
    "![CNN](cnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorboard (for use later in the notebook)\n",
    "%load_ext tensorboard\n",
    "# clear any logs from previous tensorboard runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 1000, 16)          56000     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 996, 64)           5184      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 199, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 12736)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                815168    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 876,417\n",
      "Trainable params: 876,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # adding an embedding layer for neural net to learn the vectors\n",
    "    Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    # convolutional layer\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    # pooling layer\n",
    "    MaxPooling1D(5),\n",
    "    # flattens the input, converting matrix to single array\n",
    "    # reducing tensor to a single dimension\n",
    "    Flatten(),\n",
    "    # dense layer, which means each neuron in the layer receives input from all neurons of previous layer\n",
    "    # activation function is used to map the output of one layer to another\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%m/%d -- %H:%M\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/9 [=====>........................] - ETA: 0s - loss: 0.6133 - accuracy: 0.5469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0344s vs `on_train_batch_end` time: 0.0725s). Check your callbacks.\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.4304 - accuracy: 0.7915 - val_loss: 0.7454 - val_accuracy: 0.8036\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3672 - accuracy: 0.8919 - val_loss: 0.4927 - val_accuracy: 0.8036\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.3418 - accuracy: 0.8919 - val_loss: 0.5422 - val_accuracy: 0.8036\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.3256 - accuracy: 0.8919 - val_loss: 0.5188 - val_accuracy: 0.8036\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.3108 - accuracy: 0.8919 - val_loss: 0.4940 - val_accuracy: 0.8036\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2787 - accuracy: 0.8919 - val_loss: 0.5184 - val_accuracy: 0.8036\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2560 - accuracy: 0.8919 - val_loss: 0.4949 - val_accuracy: 0.8036\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1964 - accuracy: 0.8919 - val_loss: 0.5807 - val_accuracy: 0.8036\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1680 - accuracy: 0.8919 - val_loss: 0.6275 - val_accuracy: 0.8036\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1337 - accuracy: 0.8919 - val_loss: 0.6250 - val_accuracy: 0.8036\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f9e168200a0>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_pad,\n",
    "    y_train,\n",
    "    epochs = 10,\n",
    "    validation_data = (test_pad,y_test),\n",
    "    callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6250 - accuracy: 0.8036\n",
      "Accuracy: 0.8035714030265808\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_pad, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Long Short-Term Memory Network\n",
    "\n",
    "Long Short-Term Memory (LSTM) models are a neural network that has become popular in natural language processing. Its popularity owes to the specifics of its architecture. LSTMs are designed to work on sequence data, and therefore it treats text data in the sequence that it appears (unlike other methods like Bag of Words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 1000, 16)          56000     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                20736     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 76,801\n",
      "Trainable params: 76,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    # long short-term memory layer\n",
    "    LSTM(64),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%m/%d -- %H:%M\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/9 [=====>........................] - ETA: 4s - loss: 0.6909 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4416s vs `on_train_batch_end` time: 0.8569s). Check your callbacks.\n",
      "9/9 [==============================] - 5s 569ms/step - loss: 0.6617 - accuracy: 0.7838 - val_loss: 0.6158 - val_accuracy: 0.8036\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 4s 403ms/step - loss: 0.4716 - accuracy: 0.8919 - val_loss: 0.6021 - val_accuracy: 0.8036\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 4s 447ms/step - loss: 0.3665 - accuracy: 0.8919 - val_loss: 0.5685 - val_accuracy: 0.8036\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 3s 380ms/step - loss: 0.3469 - accuracy: 0.8919 - val_loss: 0.5256 - val_accuracy: 0.8036\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 3s 361ms/step - loss: 0.3437 - accuracy: 0.8919 - val_loss: 0.5244 - val_accuracy: 0.8036\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 4s 425ms/step - loss: 0.3440 - accuracy: 0.8919 - val_loss: 0.5105 - val_accuracy: 0.8036\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 4s 396ms/step - loss: 0.3466 - accuracy: 0.8919 - val_loss: 0.5135 - val_accuracy: 0.8036\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 3s 327ms/step - loss: 0.3479 - accuracy: 0.8919 - val_loss: 0.5478 - val_accuracy: 0.8036\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 0.3446 - accuracy: 0.8919 - val_loss: 0.5441 - val_accuracy: 0.8036\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 3s 322ms/step - loss: 0.3454 - accuracy: 0.8919 - val_loss: 0.5363 - val_accuracy: 0.8036\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f9dc93f5220>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_pad,\n",
    "    y_train,\n",
    "    epochs = 10,\n",
    "    validation_data = (test_pad,y_test),\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 64ms/step - loss: 0.4981 - accuracy: 0.8036\n",
      "Accuracy: 0.8035714030265808\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_pad, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit --port 6060"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('bulgarian-constitutional-court-decisions': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "9e35bad0b5aa7a87571eaa6bfdcd491c11984b99cb58639a3f82d97f7d8977e2"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}